# AWS kOps Production-Ready Infrastructure - Cursor Rules
# Based on: AWS kOps Production-Ready Setup Guide - Extended Edition
# Target: Solo DevOps engineer building cost-effective, production-grade Kubernetes infrastructure
# Constraints: ≤ $40/month, 100% IaC, GitOps, zero snowflakes

## CORE PRINCIPLES

### Cost Optimization (Primary Constraint)
- **Budget Cap**: ≤ $40 USD/month in us-east-1
- **Spot Instances**: Use Spot for all compute (masters + workers + NAT)
- **ARM64 First**: Prefer t4g instances for ~20% cost savings
- **Custom NAT**: Use t4g.nano NAT instance instead of $32/month NAT Gateway
- **Resource Sizing**: Start minimal, scale on demand via HPA/CA
- **Cost Monitoring**: CloudWatch alarms at 80% of budget ($32)

### Technology Stack (Non-Negotiable)
- **Kubernetes**: kOps (not EKS/k3s) for exam-grade learning + cost control
- **Infrastructure**: 100% Terraform, no click-ops
- **Compute**: EC2 Spot instances across 3 AZs for HA
- **Networking**: Custom VPC, Calico CNI, IPv6-ready
- **Storage**: gp3 SSD, encrypted at rest
- **Monitoring**: Prometheus + Grafana + Loki + AlertManager
- **GitOps**: ArgoCD for application deployment
- **CI/CD**: Jenkins with security hardening

### Security (Production-Grade)
- **Default Deny**: Network policies, security groups least privilege
- **Encryption**: All data encrypted in transit + at rest
- **Secrets**: SOPS-encrypted YAML, SSM Parameter Store
- **Images**: CVE scanning with Trivy before deployment
- **Access**: No root API keys, MFA enforced, IAM roles only
- **Audit**: K8s audit logs to S3, CloudTrail enabled
- **Runtime**: Falco for anomaly detection

### High Availability Design
- **Masters**: 3 nodes across 3 AZs (t4g.small Spot)
- **Workers**: 2+ nodes with auto-scaling (t4g.small Spot)
- **etcd**: Automated S3 backups, encryption enabled
- **Networking**: Multi-AZ subnets, redundant NAT instances
- **Storage**: EBS with snapshots, cross-AZ replication

## IMPLEMENTATION RULES

### Sequential Dependencies (MUST FOLLOW ORDER)
1. **Foundation**: Terraform backend → VPC → Security Groups
2. **Compute**: NAT instance → IAM roles → kOps state store
3. **Cluster**: kOps spec → cluster deployment → validation
4. **Platform**: Ingress → cert-manager → monitoring
5. **Operations**: Logging → alerting → backup/DR
6. **CI/CD**: ArgoCD → Jenkins → GitOps workflows

### Terraform Standards
- **State**: Remote S3 backend with DynamoDB locking
- **Modules**: Reusable modules in `modules/` directory
- **Environments**: Separate `envs/dev/` and `envs/prod/`
- **Documentation**: terraform-docs for all modules
- **Validation**: tflint, terraform fmt, terraform validate
- **Secrets**: Never in .tf files, use data sources or SSM

### kOps Configuration
- **Version**: ≥ 1.29 for K8s 1.30 support
- **Networking**: Calico CNI, IPv6 dual-stack ready
- **etcd**: 3.5+ with S3 backups, encryption at rest
- **API**: Audit logging enabled, RBAC enforced
- **Nodes**: Spot instances with interruption handling
- **Addons**: metrics-server, CoreDNS autoscaler

### Kubernetes Best Practices
- **Resources**: Always set requests/limits
- **Health**: Readiness/liveness probes mandatory
- **Security**: Pod Security Standards, NetworkPolicies
- **Scaling**: HPA for apps, CA for nodes
- **Images**: No :latest tags, signed images preferred
- **Secrets**: Sealed secrets or external-secrets operator

### Monitoring & Observability
- **Metrics**: Prometheus with 15d retention
- **Dashboards**: Grafana with persistent storage
- **Logs**: Loki stack with 7d retention
- **Alerts**: AlertManager with Slack integration
- **Uptime**: Blackbox exporter for external monitoring
- **Cost**: AWS Cost Explorer integration

### Disaster Recovery
- **etcd**: Daily automated backups to S3
- **Configs**: All manifests in Git, GitOps sync
- **Runbooks**: Documented procedures for common failures
- **Testing**: Monthly DR drills, backup restoration
- **RTO**: < 30 minutes for cluster recovery
- **RPO**: < 24 hours for data loss

## VALIDATION CRITERIA

### Success Metrics (From PDF)
- **Deployment**: First cluster passes `kops validate` within 45 min
- **Cost**: Monthly AWS bill ≤ $40 for first 3 months
- **Availability**: Blue-green rolling updates < 20 min, zero 5xx errors
- **Security**: CIS Kubernetes Benchmark compliance
- **Learning**: Full etcd access, certificate rotation, troubleshooting

### Quality Gates
- **Infrastructure**: `terraform plan` shows no drift
- **Security**: No HIGH/CRITICAL CVEs in running images
- **Performance**: All pods have resource requests/limits
- **Monitoring**: All critical services have alerts configured
- **Documentation**: All procedures documented and tested

## EXAM PREPARATION ALIGNMENT

### CKA Focus Areas
- **etcd**: Backup/restore, cluster recovery procedures
- **Networking**: Calico policies, service mesh basics
- **Troubleshooting**: Node issues, pod scheduling, resource constraints
- **Security**: RBAC, Pod Security, certificate management

### CKAD Focus Areas
- **Applications**: Helm charts, resource management
- **Configuration**: ConfigMaps, Secrets, environment variables
- **Observability**: Logging, monitoring, debugging
- **Services**: Ingress, service types, network policies

### CKS Focus Areas
- **Supply Chain**: Image scanning, admission controllers
- **Runtime**: Falco rules, container security
- **Monitoring**: Audit logs, anomaly detection
- **Hardening**: CIS benchmarks, least privilege

## COST BREAKDOWN TARGET

| Resource | Qty | Unit Price $/h | Hours/mo | Cost/mo | Notes |
|----------|-----|----------------|----------|---------|-------|
| Masters (t4g.small Spot) | 3 | 0.0054 | 730 | 11.80 | 3 AZs for quorum |
| Workers (t4g.small Spot) | 2 | 0.0054 | 730 | 7.90 | HPA/CA scales up |
| NAT instance (t4g.nano Spot) | 1 | 0.0021 | 730 | 1.50 | Replaces $32 NAT-GW |
| gp3 SSD | 100 GiB | 0.08/GiB-mo | - | 8.00 | Masters + workers |
| S3, CloudWatch, Data out | - | - | - | 5.00 | Conservative buffer |
| **Total baseline** | | | | **34.20** | Leaves $5-6 for scaling |

## EMERGENCY PROCEDURES

### Cost Spike Response
1. Check AWS Budget alarm notification
2. Run `aws ce get-cost-and-usage` for detailed breakdown
3. Scale down non-essential workloads immediately
4. Review Spot instance pricing history
5. Implement temporary resource quotas

### Cluster Failure Response
1. Check `kops validate cluster` output
2. Verify etcd quorum: `kubectl -n kube-system get pods -l k8s-app=etcd-manager`
3. If etcd corrupted: restore from latest S3 backup
4. If nodes unhealthy: `kops rolling-update cluster --yes`
5. Document incident and update runbooks

Remember: Every decision must optimize for cost, security, and learning value. When in doubt, choose the option that teaches the most about production Kubernetes operations while staying within budget constraints.
